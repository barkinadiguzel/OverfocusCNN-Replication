# ğŸ”¬ OverfocusCNN-Replication PyTorch Implementation

This repository contains a replication of **Convolutional Neural Networks with Saliency-Guided Dropout (SGDrop)** using PyTorch. The goal is to build a **CNN backbone** that prevents overfocusing on highly salient regions and encourages the network to attend to diverse features.

- Implemented **CNN** with **SGDrop** for saliency-guided regularization.  
- Architecture:  
**Conv1 â†’ ResidualBlock1 â†’ ResidualBlock2 â†’ ResidualBlock3 â†’ BN â†’ ReLU â†’ AvgPool â†’ Flatten â†’ FC**  
**Paper**: [The Overfocusing Bias of CNNs: A Saliency-Guided Regularization Approach](https://arxiv.org/abs/2409.17370)

---

## ğŸ–¼ Overview â€“ OverfocusCNN with SGDrop

![Figure 1](images/figmix.jpg)  

- **Figure 1:** Standard CNN residual connections with SGDrop. Each residual block adds its input to the output via identity or projection shortcuts. SGDrop is applied to the most salient features during training to redistribute attention.  

- **Figure 2:** Conceptual illustration of SGDrop:  
  - Attribution maps identify highly salient features.  
  - Top Ï-quantile features are masked (dropped) to prevent overfocusing.  
  - This encourages the network to learn from less dominant features, improving generalization.

### ğŸ”¢ Mathematical Formulation

The saliency map and dropout are computed as:

```math
\mathcal{A}_\psi (z,c) = \text{ReLU}\left( \nabla_z \psi_c(z) \odot z \right)
```
- `z` = latent feature map extracted by the encoder Ï•  
- `Ïˆ_c(z)` = class score for class `c` from the classifier Ïˆ  
- `âˆ‡_z Ïˆ_c(z)` = gradient of class score w.r.t. latent features  
- `âŠ™` = element-wise multiplication (Hadamard product)  
- `ReLU` ensures only positive gradients are considered, similar to Grad-CAM  

The SGDrop mask is computed by:

```math
M_f(x,c,Ï) = I[\mathcal{A}_\psi(z,c) â‰¤ q_Ï]
```
Where:  
- `q_Ï` = upper Ï-quantile of attribution values in `A_Ïˆ(z,c)`  
  - This means the threshold above which the top Ï% most salient features lie.  
- `I[Â·]` = indicator function returning 1 if condition is True, 0 otherwise  
  - Used to create a binary mask that keeps less salient features (1) and drops the most salient ones (0).  
- `xÌƒ = x âŠ™ M_f(x,c,Ï)` gives the saliency-regularized features  
  - The original features `x` are element-wise multiplied by the mask to drop the highly salient ones.

> In short: SGDrop **selectively drops the top Ï% most salient features** during training. This forces the network to diversify its attention and not rely solely on narrow, high-importance regions, improving generalization.

---
## ğŸ— Project Structure

```bash
OverfocusCNN-Replication/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py
â”‚   â”‚   â”œâ”€â”€ residual_block.py
â”‚   â”‚   â”œâ”€â”€ shortcut_layer.py
â”‚   â”‚   â”œâ”€â”€ saliency_regularization.py
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py
â”‚   â”‚   â”œâ”€â”€ flatten_layer.py
â”‚   â”‚   â””â”€â”€ fc_layer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ overfocus_cnn.py
â”‚   â”‚
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
